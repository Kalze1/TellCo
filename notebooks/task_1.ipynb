{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.load_data import load_df\n",
    "df = load_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "\n",
    "# Melt the DataFrame to have a single 'application' column and separate 'DL' and 'UL' volumes\n",
    "melted_df = pd.melt(df, \n",
    "                    id_vars=[], \n",
    "                    value_vars=['Social Media DL (Bytes)', 'Social Media UL (Bytes)',\n",
    "                                'Google DL (Bytes)', 'Google UL (Bytes)',\n",
    "                                'Email DL (Bytes)', 'Email UL (Bytes)',\n",
    "                                'Youtube DL (Bytes)', 'Youtube UL (Bytes)',\n",
    "                                'Netflix DL (Bytes)', 'Netflix UL (Bytes)',\n",
    "                                'Gaming DL (Bytes)', 'Gaming UL (Bytes)',\n",
    "                                'Other DL (Bytes)', 'Other UL (Bytes)'],\n",
    "                    var_name='application_type', \n",
    "                    value_name='data_volume')\n",
    "\n",
    "# Split 'application_type' into 'application' and 'type' (DL/UL)\n",
    "# Adjust this logic to ensure it correctly splits application and type.\n",
    "melted_df['application'] = melted_df['application_type'].apply(lambda x: ' '.join(x.split(' ')[:-2]))\n",
    "melted_df['type'] = melted_df['application_type'].apply(lambda x: x.split(' ')[-2])\n",
    "\n",
    "# Pivot to get the correct format for aggregation\n",
    "pivot_df = melted_df.pivot_table(index='application', columns='type', values='data_volume', aggfunc='sum').reset_index()\n",
    "\n",
    "# Inspect the columns to check the structure\n",
    "print(pivot_df.columns)\n",
    "\n",
    "# Rename the columns based on the actual structure\n",
    "# This should now correctly map the columns\n",
    "pivot_df.columns = ['application', 'Total DL (Bytes)', 'Total UL (Bytes)']\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(pivot_df['application'], pivot_df['Total DL (Bytes)'], label='Download')\n",
    "plt.bar(pivot_df['application'], pivot_df['Total UL (Bytes)'], label='Upload', bottom=pivot_df['Total DL (Bytes)'])\n",
    "plt.xlabel('Application')\n",
    "plt.ylabel('Data Volume (Bytes)')\n",
    "plt.title('Total Data Volume per Application')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "aggregated_data = df.groupby('application').agg({'Total DL (Bytes)': 'sum', 'Total UL (Bytes)': 'sum'}).reset_index()\n",
    "\n",
    "# Create a stacked bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(aggregated_data['application'], aggregated_data['Total DL (Bytes)'], label='Download')\n",
    "plt.bar(aggregated_data['application'], aggregated_data['Total UL (Bytes)'], label='Upload', bottom=aggregated_data['Total DL (Bytes)'])\n",
    "plt.xlabel('Application')\n",
    "plt.ylabel('Data Volume (Bytes)')\n",
    "plt.title('Total Data Volume per Application')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.preprocessing import count_missing_values, replace_missing_with_mean, count_outliers, replace_outliers_with_percentile\n",
    "\n",
    "mv = count_missing_values(df)\n",
    "columns = ['Dur. (ms)', 'Avg RTT DL (ms)',\n",
    "       'Avg RTT UL (ms)', 'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)',\n",
    "       'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)',\n",
    "       'DL TP < 50 Kbps (%)', '50 Kbps < DL TP < 250 Kbps (%)',\n",
    "       '250 Kbps < DL TP < 1 Mbps (%)', 'DL TP > 1 Mbps (%)',\n",
    "       'UL TP < 10 Kbps (%)', '10 Kbps < UL TP < 50 Kbps (%)',\n",
    "       '50 Kbps < UL TP < 300 Kbps (%)', 'UL TP > 300 Kbps (%)',\n",
    "       'HTTP DL (Bytes)', 'HTTP UL (Bytes)', 'Activity Duration DL (ms)',\n",
    "       'Activity Duration UL (ms)', 'Dur. (ms).1', 'Nb of sec with 125000B < Vol DL',\n",
    "       'Nb of sec with 1250B < Vol UL < 6250B',\n",
    "       'Nb of sec with 31250B < Vol DL < 125000B',\n",
    "       'Nb of sec with 37500B < Vol UL',\n",
    "       'Nb of sec with 6250B < Vol DL < 31250B',\n",
    "       'Nb of sec with 6250B < Vol UL < 37500B',\n",
    "       'Nb of sec with Vol DL < 6250B', 'Nb of sec with Vol UL < 1250B',\n",
    "       'Social Media DL (Bytes)', 'Social Media UL (Bytes)',\n",
    "       'Google DL (Bytes)', 'Google UL (Bytes)', 'Email DL (Bytes)',\n",
    "       'Email UL (Bytes)', 'Youtube DL (Bytes)', 'Youtube UL (Bytes)',\n",
    "       'Netflix DL (Bytes)', 'Netflix UL (Bytes)', 'Gaming DL (Bytes)',\n",
    "       'Gaming UL (Bytes)', 'Other DL (Bytes)', 'Other UL (Bytes)',\n",
    "       'Total UL (Bytes)', 'Total DL (Bytes)']\n",
    "\n",
    "\n",
    "missing =  count_outliers(df, columns)\n",
    "print(missing)\n",
    "\n",
    "df = replace_missing_with_mean(df, columns)\n",
    "\n",
    "df=replace_outliers_with_percentile(df,columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from scripts.eda_module import (\n",
    "    load_and_inspect_data,\n",
    "    handle_missing_and_outliers,\n",
    "    segment_users_by_duration,\n",
    "    compute_basic_metrics,\n",
    "    univariate_analysis,\n",
    "    bivariate_analysis,\n",
    "    correlation_analysis,\n",
    "    pca_analysis,\n",
    "    aggregate_user_data  \n",
    ")\n",
    "\n",
    "# Call the aggregation function\n",
    "user_aggregates = aggregate_user_data(df)\n",
    "\n",
    "# Preview the result\n",
    "print(user_aggregates.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing =  count_outliers(df, columns)\n",
    "print(missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scripts.eda_module import (\n",
    "    load_and_inspect_data,\n",
    "    segment_users_by_duration,\n",
    "    compute_basic_metrics,\n",
    "    univariate_analysis,\n",
    "    bivariate_analysis,\n",
    "    correlation_analysis,\n",
    "    pca_analysis,\n",
    "    aggregate_user_data\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Inspect data\n",
    "load_and_inspect_data(df)\n",
    "\n",
    "\n",
    "# Call the aggregation function\n",
    "user_aggregates = aggregate_user_data(df)\n",
    "\n",
    "# Preview the result\n",
    "print(user_aggregates.head())\n",
    "\n",
    "# Segment users by session duration decile class\n",
    "df, decile_data = segment_users_by_duration(df)\n",
    "print(decile_data)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "decile_data.plot(kind='bar', color='skyblue')\n",
    "plt.xlabel('Decile Class')\n",
    "plt.ylabel('Total Data (Bytes)')\n",
    "plt.title('Total Data Volume per Decile Class Based on Session Duration')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "# Compute basic metrics\n",
    "mean_values, median_values, std_dev, dispersion_params = compute_basic_metrics(df)\n",
    "print(mean_values, median_values, std_dev, dispersion_params)\n",
    "\n",
    "# Perform univariate analysis\n",
    "univariate_analysis(df, 'Total_Session_Duration')\n",
    "\n",
    "# Bivariate analysis between application data and total data\n",
    "app_columns = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)', \n",
    "               'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']\n",
    "bivariate_analysis(df, app_columns)\n",
    "\n",
    "#Correlation analysis\n",
    "correlation_matrix = correlation_analysis(df, app_columns)\n",
    "\n",
    "# Perform PCA for dimensionality reduction\n",
    "pca_result = pca_analysis(df, app_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scripts.user_engagement import (\n",
    "    aggregate_engagement_metrics,\n",
    "    normalize_and_cluster,\n",
    "    compute_cluster_stats,\n",
    "    aggregate_traffic_per_app,\n",
    "    plot_top_3_apps,\n",
    "    elbow_method,\n",
    "    silhouette_analysis\n",
    ")\n",
    "\n",
    "\n",
    "# Aggregate engagement metrics\n",
    "user_metrics = aggregate_engagement_metrics(df)\n",
    "print(user_metrics.head())\n",
    "\n",
    "# Normalize and apply k-means clustering\n",
    "user_metrics, kmeans = normalize_and_cluster(user_metrics, k=3)\n",
    "print(user_metrics.head())\n",
    "\n",
    "# Compute cluster stats\n",
    "cluster_stats = compute_cluster_stats(user_metrics)\n",
    "print(cluster_stats)\n",
    " \n",
    "# Aggregate total traffic per application\n",
    "app_traffic = aggregate_traffic_per_app(df)\n",
    "print(app_traffic)\n",
    "\n",
    "# Plot top 3 most used applications\n",
    "plot_top_3_apps(app_traffic)\n",
    "\n",
    "# Use elbow method to find optimal k\n",
    "inertia = elbow_method(user_metrics)\n",
    "\n",
    "# Evaluate silhouette score for k=3\n",
    "silhouette_score_k3 = silhouette_analysis(user_metrics, k=3)\n",
    "print(f\"Silhouette Score for k=3: {silhouette_score_k3}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['MSISDN/Number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [\n",
    "        'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)',\n",
    "        'Avg RTT DL (ms)', 'Avg RTT UL (ms)',\n",
    "        'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.customer_satisfaction_analysis import aggregate_per_customer\n",
    "result_df = aggregate_per_customer(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.customer_satisfaction_analysis import compute_top_bottom_frequent_main\n",
    "results = compute_top_bottom_frequent_main(df)\n",
    "\n",
    "for col, stats in results.items():\n",
    "    print(f\"Column: {col}\")\n",
    "    print(\"Top Values:\\n\", stats['Top Values'])\n",
    "    print(\"Bottom Values:\\n\", stats['Bottom Values'])\n",
    "    print(\"Most Frequent Values:\\n\", stats['Most Frequent Values'])\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.customer_satisfaction_analysis import analyze_throughput_tcp_retransmission\n",
    "handset_stats = analyze_throughput_tcp_retransmission(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.customer_satisfaction_analysis import perform_kmeans_clustering\n",
    "cluster_descriptions, cluster_counts = perform_kmeans_clustering(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.customer_satisfaction_analysis import assign_engagement_experience_scores\n",
    "engagement_metrics = ['Metric1', 'Metric2', 'Metric3']  # Replace with actual engagement metrics\n",
    "experience_metrics = ['Avg RTT DL (ms)', 'Avg RTT UL (ms)', 'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)',\n",
    "                      'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)']  # Replace with actual experience metrics\n",
    "df = pd.read_csv('your_dataset.csv')  # Load your dataset\n",
    "scores_df = assign_engagement_experience_scores(df, engagement_metrics, experience_metrics)\n",
    "\n",
    "print(scores_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.customer_satisfaction_analysis import compute_satisfaction_score\n",
    "top_satisfied_customers = compute_satisfaction_score(df)\n",
    "\n",
    "print(top_satisfied_customers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.customer_satisfaction_analysis import build_regression_model\n",
    "features = ['Engagement Score', 'Experience Score']  \n",
    "target = 'Satisfaction Score'\n",
    "model, mse, r2 = build_regression_model(df, features, target)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.customer_satisfaction_analysis import compute_satisfaction_score\n",
    "top_satisfied_customers = compute_satisfaction_score(df)\n",
    "\n",
    "print(top_satisfied_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = ['Engagement Score', 'Experience Score'] \n",
    "target = 'Satisfaction Score'\n",
    "model, mse, r2 = build_regression_model(df, features, target)\n",
    "\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.customer_satisfaction_analysis import run_kmeans_on_scores\n",
    "df, kmeans = run_kmeans_on_scores(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.customer_satisfaction_analysis import aggregate_scores_per_cluster\n",
    "cluster_aggregation = aggregate_scores_per_cluster(df)\n",
    "print(cluster_aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.customer_satisfaction_analysis import export_to_mysql\n",
    "export_message = export_to_mysql(df, 'user_scores', 'your_user', 'your_password', 'localhost', 'your_database')\n",
    "print(export_message)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TellCoenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
